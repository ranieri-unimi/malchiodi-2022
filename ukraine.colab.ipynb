{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ranieri-unimi/malchiodi-2022/blob/main/ukraine.colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgQoZYRAWKCK"
      },
      "source": [
        "### run once"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "!pip install findspark"
      ],
      "metadata": {
        "id": "2uhwYor6WVuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mH3gc9TWKCM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"KAGGLE_USERNAME\"] = 'ranieriunimi'\n",
        "os.environ[\"KAGGLE_KEY\"] = str(hex(232307088475198570779809482024044346960))[2:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngXoXE4KWKCN"
      },
      "outputs": [],
      "source": [
        "ref = 'bwandowando/ukraine-russian-crisis-twitter-dataset-1-2-m-rows'\n",
        "!mkdir datasets\n",
        "!kaggle datasets download $ref --unzip -p ./datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ukraine"
      ],
      "metadata": {
        "id": "oAR7LusA5ZOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SAMPLE_SIZE = 512"
      ],
      "metadata": {
        "id": "c1FjJN8F6ejg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqGrV6XdWKCI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "import re\n",
        "import string\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "import findspark"
      ],
      "metadata": {
        "id": "NrOTb9eEWSpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### clearing a bit of stuff"
      ],
      "metadata": {
        "id": "i3RapMVFenPc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIzYGa4CWKCQ"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kywqu11WKCQ"
      },
      "outputs": [],
      "source": [
        "nltk.download('all');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5i9KWOCjWKCR"
      },
      "outputs": [],
      "source": [
        "# https://www.pluralsight.com/guides/building-a-twitter-sentiment-analysis-in-python\n",
        "\n",
        "def preprocess_tweet_text(tweet):\n",
        "    index, tweet = tweet\n",
        "\n",
        "    tweet.lower()\n",
        "\n",
        "    # cleanings ðŸ§¹\n",
        "\n",
        "    # urls\n",
        "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags=re.MULTILINE)\n",
        "\n",
        "    # @ and #\n",
        "    tweet = re.sub(r'\\@\\w+|\\#','', tweet)\n",
        "\n",
        "    # punctuations\n",
        "    # tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
        "    tweet = tweet.translate(str.maketrans(string.punctuation+'â€¦â€™', ' '*(len(string.punctuation)+2)))  # puntctuation to spaces\n",
        "\n",
        "\n",
        "    tweet_tokens = word_tokenize(tweet)\n",
        "\n",
        "    # emojitter\n",
        "    wrds = [e for word in tweet_tokens for e in re.findall(r\"(\\w+|[^\\w ]+)\", word)]\n",
        "    # TODO split also emoji-goups\n",
        "\n",
        "    # stopwords\n",
        "    filtered_words = [w for w in wrds if not w in set(stopwords.words('english'))]\n",
        "    \n",
        "    # stemmatize\n",
        "    ps = PorterStemmer()\n",
        "    stemmed_words = [ps.stem(w) for w in filtered_words]\n",
        "\n",
        "    # lemmatize\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemma_words = [lemmatizer.lemmatize(w, pos='a') for w in stemmed_words]\n",
        "  \n",
        "    return (index, lemma_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoA9_EouWKCO"
      },
      "outputs": [],
      "source": [
        "# load dataset \n",
        "filename = r\"./datasets/UkraineCombinedTweetsDeduped20220227-131611.csv.gzip\"\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "df = pd.read_csv(filename, compression='gzip', index_col=0, encoding='utf-8', quoting=csv.QUOTE_ALL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABxWwFYYWKCP"
      },
      "outputs": [],
      "source": [
        "#lang_hist = {l:df[df.language == l].size for l in df.language.unique()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_frlylt5WKCP"
      },
      "outputs": [],
      "source": [
        "datalist = df[df.language == 'en'].text.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if SAMPLE_SIZE:\n",
        "    datalist = random.sample(datalist, SAMPLE_SIZE)"
      ],
      "metadata": {
        "id": "FCWDtxA26udo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REaNAuF2WKCS"
      },
      "source": [
        "### hadoooooop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gMPGu1YWKCS"
      },
      "outputs": [],
      "source": [
        "# import findspark\n",
        "# findspark.init(\"spark-3.1.1-bin-hadoop3.2\") # SPARK_HOME\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize(enumerate(datalist))"
      ],
      "metadata": {
        "id": "_Kll5uQHB5hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataPipe = rdd.map(preprocess_tweet_text)"
      ],
      "metadata": {
        "id": "8TJTiM7aB-vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# indexing = dataset.flatMap(lambda _, v : [(e,e) for e in v]).reduceByKey(lambda k, v : k)"
      ],
      "metadata": {
        "id": "qweNpiR__zu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### alternative"
      ],
      "metadata": {
        "id": "5KLgYMu3DFUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nen_rdd = sc.parallelize(datalist)"
      ],
      "metadata": {
        "id": "n4z4knWFC7bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# elemListPipe = nen_rdd.flatMap(lambda word_list : word_list).distinct()\n",
        "# elem_index = sc.parallelize(enumerate(elemListPipe.collect()))"
      ],
      "metadata": {
        "id": "yye2c9EaDIx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1ne"
      ],
      "metadata": {
        "id": "fvv0WIpO-Tma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_sub(sub, lst) : return all(e in lst for e in sub)\n",
        "def add(a, b) : return a+b\n",
        "def splat(t): return tuple(sorted(list(j for i in t for j in (i if isinstance(i, tuple) else (i,)))))\n",
        "def doubled(t): return len(set(t)) == len(t)"
      ],
      "metadata": {
        "id": "5yG0xwKXOhzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PCENT = .005"
      ],
      "metadata": {
        "id": "e5JdaqEKAg3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "candidateOne = dataPipe.flatMap(lambda x: x[-1]).distinct().collect()"
      ],
      "metadata": {
        "id": "SObjzZhzRzIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "countOnePipe = dataPipe.flatMap(lambda x: x[-1]).map(lambda x: (x,1)).reduceByKey(add)\n",
        "countOnePipe.take(5)"
      ],
      "metadata": {
        "id": "zNeFbxU8zMCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "THRESHOLD = countOnePipe.map(lambda x : x[-1]).reduce(lambda a,b : a+b) * PCENT"
      ],
      "metadata": {
        "id": "GpOzBbqPKUac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter non frequent\n",
        "frequentOnePipe = countOnePipe.filter(lambda x: x[-1] > THRESHOLD).map(lambda x: (1, x[0]))\n",
        "frequentOnePipe.take(5)"
      ],
      "metadata": {
        "id": "HpIN3wm5AO8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2wo"
      ],
      "metadata": {
        "id": "ktVuc_98_vb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate candidate pairs\n",
        "candidateTwoPipe = frequentOnePipe.join(frequentOnePipe).map(lambda x : x[-1]).map(splat).distinct().filter(doubled)\n",
        "candidateTwoPipe.take(5)"
      ],
      "metadata": {
        "id": "I2EfKt2ULdyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count pair frequency\n",
        "candidateTwo = candidateTwoPipe.collect() # pair in MEM\n",
        "\n",
        "countTwoPipe = dataPipe.map(lambda x : [(pair, is_sub(pair, x[-1])) for pair in candidateTwo] ).flatMap(lambda x : x).reduceByKey(add)\n",
        "countTwoPipe.take(5)"
      ],
      "metadata": {
        "id": "j4IyyuSJNtBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#THRESHOLD = countTwoPipe.map(lambda x : x[-1]).reduce(lambda a,b : a+b) * PCENT"
      ],
      "metadata": {
        "id": "J4cPjz-4VwhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter non frequent\n",
        "frequentTwoPipe = countTwoPipe.filter(lambda x: x[-1] > THRESHOLD).map(lambda x: (1, x[0]))\n",
        "frequentTwoPipe.take(5)"
      ],
      "metadata": {
        "id": "r42SRbQpT3hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3hree"
      ],
      "metadata": {
        "id": "g_RSS8BK_poo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate candidate triples\n",
        "candidateThreePipe = frequentTwoPipe.join(frequentOnePipe).map(lambda x : x[-1]).map(splat).distinct().filter(doubled)\n",
        "candidateThreePipe.take(5)"
      ],
      "metadata": {
        "id": "y0o2kYU5YWu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count three frequency\n",
        "candidateThree = candidateThreePipe.collect() # three in MEM\n",
        "\n",
        "countThreePipe = dataPipe.map(lambda x : [(pair, is_sub(pair, x[-1])) for pair in candidateThree] ).flatMap(lambda x : x).reduceByKey(add)\n",
        "countThreePipe.take(5)"
      ],
      "metadata": {
        "id": "qT1vY7vBKTlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter non frequent\n",
        "frequentThreePipe = countThreePipe.filter(lambda x: x[-1] > THRESHOLD).map(lambda x: (1, x[0]))\n",
        "frequentThreePipe.take(5)"
      ],
      "metadata": {
        "id": "7sClHGUjKfkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataPipe.count()"
      ],
      "metadata": {
        "id": "tV25-KRAYxfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### let's generalize thot shit"
      ],
      "metadata": {
        "id": "a54RokrvRb6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def aPriori(data, THRESHOLD = .1):\n",
        "  THRESHOLD *= data.count()\n",
        "\n",
        "  frequent_items_pipe = (data\n",
        "                    .flatMap(lambda x: x[-1])\n",
        "                    .map(lambda x: (x,1))\n",
        "                    .reduceByKey(add)\n",
        "                    .filter(lambda x: x[-1] > THRESHOLD)\n",
        "                    )\n",
        "\n",
        "  frequent_items = frequent_items_pipe.collect()\n",
        "  frequent_items_pipe = frequent_items_pipe.map(lambda x: (1, x[0]))\n",
        "\n",
        "  frequent_itemsets = frequent_items\n",
        "  frequent_itemsets_pipe = frequent_items_pipe\n",
        "\n",
        "  while len(frequent_itemsets):\n",
        "    yield frequent_itemsets\n",
        "    ### COUNTING PHASE\n",
        "    candidate_itemsets_pipe = (frequent_itemsets_pipe\n",
        "                          .join(frequent_items_pipe)\n",
        "                          .map(lambda x : x[-1])\n",
        "                          .map(splat)\n",
        "                          .distinct()\n",
        "                          .filter(doubled)\n",
        "                          )\n",
        "    \n",
        "    candidate_itemsets = candidate_itemsets_pipe.collect()\n",
        "\n",
        "    ### FILTER PHASE\n",
        "    frequent_itemsets_pipe = (data\n",
        "                         .map(lambda x : [(pair, is_sub(pair, x[-1])) for pair in candidate_itemsets] )\n",
        "                         .flatMap(lambda x : x)\n",
        "                         .reduceByKey(add)\n",
        "                         .filter(lambda x: x[-1] > THRESHOLD)\n",
        "                         )\n",
        "    \n",
        "    frequent_itemsets = frequent_itemsets_pipe.collect()\n",
        "    frequent_itemsets_pipe = frequent_itemsets_pipe.map(lambda x: (1, x[0]))"
      ],
      "metadata": {
        "id": "CbbfHuvLRkmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fgen = aPriori(dataPipe, 0.05)"
      ],
      "metadata": {
        "id": "LFyS87T7XPiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(fgen) # twice or trice"
      ],
      "metadata": {
        "id": "8G4ucj_7ZXrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### example with SON"
      ],
      "metadata": {
        "id": "ustF6zLee1Bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO "
      ],
      "metadata": {
        "id": "aQc2HcvXe4PL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a bit of confidence and support"
      ],
      "metadata": {
        "id": "ob02rugbe7UG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "9KzaKPVafAYS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "e2cd697ea1a65fb50dbc24e0f8485a2ca518dbc7d7a135f655e94e4f59b8539e"
    },
    "kernelspec": {
      "display_name": "Python 3.10.2 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "ukraine.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}